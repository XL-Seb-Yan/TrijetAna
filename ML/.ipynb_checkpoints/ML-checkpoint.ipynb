{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fd246c5efc49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmplhep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mROOT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplhep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "infile_QCD = util.load(\"Data_selected_raw_2017QCD.coffea\")\n",
    "infile_Sig = util.load(\"Data_selected_raw_2017Res1ToRes2GluTo3Glu.coffea\")\n",
    "display(infile_QCD)\n",
    "display(infile_Sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "var2use = [var for var in infile_QCD.keys()]\n",
    "var2use.pop(0)\n",
    "var2use.pop(0)\n",
    "display(var2use)\n",
    "\n",
    "# Import dict_accumulator into padas\n",
    "QCD_samples = [x for x in infile_QCD[\"total_events\"].keys()]\n",
    "QCD_dict = {}\n",
    "QCD_df_dict_train = {}\n",
    "QCD_df_dict_test = {}\n",
    "Sig_samples = []\n",
    "# only use signals with M > 2 TeV, note removing samples from a list is bugged, since when removing elements, it is possible for the code to\n",
    "# skipped some elements\n",
    "for x in infile_Sig[\"total_events\"].keys():\n",
    "    if any([n in x for n in [\"-500_\",\"-750_\",\"-1000_\"]]):\n",
    "        continue\n",
    "    Sig_samples.append(x)\n",
    "display(Sig_samples)\n",
    "Sig_dict = {}\n",
    "Sig_df_dict_train = {}\n",
    "Sig_df_dict_test = {}\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "for sample in QCD_samples:\n",
    "    print(\"Reading data for: \", sample)\n",
    "    QCD_dict[sample] = {}\n",
    "    for var in var2use:\n",
    "        QCD_dict[sample][var] = infile_QCD[var][sample].value\n",
    "    df_temp = pd.DataFrame(data=QCD_dict[sample])\n",
    "    df_temp_set1, df_temp_set2 = train_test_split(df_temp, train_size=0.02, random_state=1)\n",
    "    # We seperate dataset here manually so it is earlier to calculate the sentivity in the end (which needs us to reweight the QCD files)\n",
    "    QCD_df_dict_train[sample] = df_temp_set1\n",
    "    QCD_df_dict_test[sample] = df_temp_set2\n",
    "        \n",
    "for sample in Sig_samples:\n",
    "    print(\"Reading data for: \", sample)\n",
    "    Sig_dict[sample] = {}\n",
    "    for var in var2use:\n",
    "        Sig_dict[sample][var] = infile_Sig[var][sample].value\n",
    "    df_temp = pd.DataFrame(data=Sig_dict[sample])\n",
    "    df_temp_set1, df_temp_set2 = train_test_split(df_temp, train_size=0.25, random_state=1)\n",
    "    Sig_df_dict_train[sample] = df_temp_set1\n",
    "    Sig_df_dict_test[sample] = df_temp_set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "QCD_df_list = [x for x in QCD_df_dict_train.keys()]\n",
    "Sig_df_list = [x for x in Sig_df_dict_train.keys()]\n",
    "QCD_df_train = pd.concat([QCD_df_dict_train[x] for x in QCD_df_list])\n",
    "Sig_df_train = pd.concat([Sig_df_dict_train[x] for x in Sig_df_list])\n",
    "QCD_df_test = pd.concat([QCD_df_dict_test[x] for x in QCD_df_list])\n",
    "Sig_df_test = pd.concat([Sig_df_dict_test[x] for x in Sig_df_list])\n",
    "print(QCD_df_train.head(), Sig_df_train.head(), QCD_df_test.head(), Sig_df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare lables for dataframes, please note we already spplited the data\n",
    "QCD_labels = np.zeros(QCD_df_train.shape[0])\n",
    "Sig_labels = np.ones(Sig_df_train.shape[0])\n",
    "\n",
    "X_train = pd.concat([QCD_df_train, Sig_df_train])\n",
    "y_train = np.append(QCD_labels, Sig_labels)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "# scaled_df = preprocessing.scale(X_train)\n",
    "scaled_df = X_train\n",
    "pca = PCA(0.95)\n",
    "pca.fit(scaled_df)\n",
    "PCA_df = pca.transform(scaled_df)\n",
    "PCA_df = pd.DataFrame(data = PCA_df[:,:2],columns = [\"PC1\",\"PC2\"])\n",
    "PCA_df_final = pd.concat([PCA_df,pd.DataFrame(y_train, columns=[\"labels\"])] ,axis = 1)\n",
    "display(PCA_df_final.head())\n",
    "# Calculate percentage variation\n",
    "per_var = np.round(pca.explained_variance_ratio_ * 100, decimals = 1)\n",
    "# Calculate loading scores, determine best variables of each PCs\n",
    "for i in range(0, len(per_var)):\n",
    "    loading_scores = pd.Series(pca.components_[i], index=var2use)\n",
    "    loading_scores = loading_scores.abs().sort_values(ascending=False)\n",
    "    display(loading_scores[:3])\n",
    "labels = [\"PC\" + str(x) for x in range(1, len(per_var)+1)]\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(x=range(1, len(per_var)+1), height=per_var, tick_label=labels)\n",
    "plt.ylabel(\"% of explained variance\")\n",
    "plt.xlabel(\"PC\")\n",
    "plt.xticks(rotation=270)\n",
    "plt.show()\n",
    "display(pca.components_)\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = [0,1]\n",
    "colors = [\"b\",\"r\"]\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = PCA_df_final[\"labels\"] == target\n",
    "    ax.scatter(PCA_df_final.loc[indicesToKeep, \"PC1\"],\n",
    "               PCA_df_final.loc[indicesToKeep, \"PC2\"],\n",
    "               c = color,\n",
    "               s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EmpiricalCovariance\n",
    "import seaborn as sns\n",
    "empcov = EmpiricalCovariance(assume_centered=True)\n",
    "cov = empcov.fit(X_train)\n",
    "display(cov.covariance_)\n",
    "var_names = QCD_df_train.columns\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (14,10))\n",
    "sns.heatmap(cov.covariance_, ax=ax, vmin=0, vmax=1, xticklabels=var_names, yticklabels=var_names, cmap=\"viridis\")\n",
    "ax.axes.set_title(\"Covariance\",fontsize=20)\n",
    "ax.tick_params(labelsize=10)\n",
    "plt.xticks(rotation=270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='adam',hidden_layer_sizes=(20,20),random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Number of QCD events used for training (first split): \", QCD_df_train.shape[0])\n",
    "print(\"Number of Sig events used for training (first split): \", Sig_df_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually calculate classificiation efficiency\n",
    "sig_index_list = np.where(y_test == 0)[0].tolist()\n",
    "N_test = len(sig_index_list)\n",
    "clf_results_sig = np.array([])\n",
    "N_correct = 0\n",
    "N_tested = 0\n",
    "for i, index in enumerate(sig_index_list):\n",
    "    if i % 20000 == 0:\n",
    "        print(f\"Reading: {i} out of {N_test}\")\n",
    "    is_correct = (y_test[index] == clf.predict(X_test[index:index+1, :])[0]) \n",
    "    if is_correct:\n",
    "        clf_results_sig = np.append(clf_results_sig, 1)\n",
    "    else:\n",
    "        clf_results_sig = np.append(clf_results_sig, 0)\n",
    "    # save results and restart again to speed things up\n",
    "    if i % 20000 == 0:\n",
    "        N_correct += np.sum(clf_results_sig)\n",
    "        N_tested += np.size(clf_results_sig)\n",
    "        print(N_correct, N_tested)\n",
    "        clf_results_sig = np.array([])\n",
    "# Save the results from last batch\n",
    "N_correct += np.sum(clf_results_sig)\n",
    "N_tested += np.size(clf_results_sig)\n",
    "# Calculate classification efficiency\n",
    "print(N_correct, N_tested, N_correct / N_tested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the fit using the pure QCD or Signal data\n",
    "QCD_labels = np.zeros(QCD_df_test.shape[0])\n",
    "Sig_labels = np.ones(Sig_df_test.shape[0])\n",
    "QCD_test_scaled = scaler.transform(QCD_df_test)\n",
    "print(clf.score(QCD_test_scaled,QCD_labels))\n",
    "Sig_test_scaled = scaler.transform(Sig_df_test)\n",
    "print(clf.score(Sig_test_scaled,Sig_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Implement weight of MCs\n",
    "lumi = 41.53*1000\n",
    "weight_xsec = {\n",
    "    'QCD_Pt_300to470': lumi*6830/infile_QCD[\"total_events\"][\"QCD_Pt_300to470\"],\n",
    "    'QCD_Pt_470to600': lumi*552.1/infile_QCD[\"total_events\"][\"QCD_Pt_470to600\"],\n",
    "    'QCD_Pt_600to800': lumi*156.5/infile_QCD[\"total_events\"][\"QCD_Pt_600to800\"],\n",
    "    'QCD_Pt_800to1000': lumi*26.28/infile_QCD[\"total_events\"][\"QCD_Pt_800to1000\"],\n",
    "    'QCD_Pt_1000to1400': lumi*7.47/infile_QCD[\"total_events\"][\"QCD_Pt_1000to1400\"],\n",
    "    'QCD_Pt_1400to1800': lumi*0.6484/infile_QCD[\"total_events\"][\"QCD_Pt_1400to1800\"],\n",
    "    'QCD_Pt_1800to2400': lumi*0.08743/infile_QCD[\"total_events\"][\"QCD_Pt_1800to2400\"],\n",
    "    'QCD_Pt_2400to3200': lumi*0.005236/infile_QCD[\"total_events\"][\"QCD_Pt_2400to3200\"],\n",
    "    'QCD_Pt_3200toInf': lumi*0.0001357/infile_QCD[\"total_events\"][\"QCD_Pt_3200toInf\"],\n",
    "}\n",
    "weight_sig = {}\n",
    "for name, N_evt in infile_Sig[\"total_events\"].items():\n",
    "    weight_sig[name] = lumi/N_evt\n",
    "weight_xsec.update(weight_sig)\n",
    "display(weight_xsec)\n",
    "\n",
    "# Get the total number of events being selected by HLT and pre-selection\n",
    "N_QCD = 0\n",
    "for sample, N_selected in infile_QCD[\"selected_events\"].items():\n",
    "    N_QCD += N_selected\n",
    "N_Sig = 0\n",
    "for sample, N_selected in infile_Sig[\"selected_events\"].items():\n",
    "    N_Sig += N_selected\n",
    "    \n",
    "# Calculate corrections need to be applied into S/sqrt(B), since we only use half of the data for calculation\n",
    "ratio_bkg = QCD_df_test.shape[0] / N_QCD\n",
    "ratio_sig = Sig_df_test.shape[0] / N_Sig\n",
    "print(ratio_bkg, ratio_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "N_bkg_tagged = 0\n",
    "for sample in QCD_df_dict_test.keys():\n",
    "    weight = weight_xsec[sample]\n",
    "    bkg_scaled = scaler.transform(QCD_df_dict_test[sample])\n",
    "    bkg_result = clf.predict(bkg_scaled)\n",
    "    N_bkg_tagged += np.sum(bkg_result) * (weight / ratio_bkg)\n",
    "print(N_bkg_tagged)\n",
    "\n",
    "N_sig_tagged = 0\n",
    "for sample in [\"Res1ToRes2GluTo3Glu_M1-2000_R-0p3\"]:\n",
    "    weight = weight_xsec[sample]\n",
    "    sig_scaled = scaler.transform(Sig_df_dict_test[sample])\n",
    "    sig_result = clf.predict(sig_scaled)\n",
    "    N_sig_tagged += np.sum(sig_result) * (weight / ratio_sig)\n",
    "print(N_sig_tagged)\n",
    "print(N_sig_tagged/math.sqrt(N_bkg_tagged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "filename = './MLP_QCD_ggg_15_15.pkl'\n",
    "joblib.dump(clf, filename, compress=9)\n",
    "\n",
    "clf2 = joblib.load(filename)\n",
    "clf2.score(Sig_set2_scaled, Sig_label_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
